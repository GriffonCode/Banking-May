{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Random Forest\n",
    "\n",
    "### 3.1 Load data\n",
    "\n",
    "We load our and display output from data preparation section and continue to our first model, based on logistic regression. (See notebook \"loan_default_data_preparation\" for details on the data generation and distributions)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_d930c_ th {\n",
       "  text-align: left;\n",
       "}\n",
       "#T_d930c_row0_col0, #T_d930c_row0_col1, #T_d930c_row0_col2, #T_d930c_row1_col0, #T_d930c_row1_col1, #T_d930c_row1_col2, #T_d930c_row2_col0, #T_d930c_row2_col1, #T_d930c_row2_col2, #T_d930c_row3_col0, #T_d930c_row3_col1, #T_d930c_row3_col2, #T_d930c_row4_col0, #T_d930c_row4_col1, #T_d930c_row4_col2, #T_d930c_row5_col0, #T_d930c_row5_col1, #T_d930c_row5_col2, #T_d930c_row6_col0, #T_d930c_row6_col1, #T_d930c_row6_col2, #T_d930c_row7_col0, #T_d930c_row7_col1, #T_d930c_row7_col2, #T_d930c_row8_col0, #T_d930c_row8_col1, #T_d930c_row8_col2 {\n",
       "  text-align: left;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_d930c_\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th class=\"col_heading level0 col0\" >Feature</th>\n",
       "      <th class=\"col_heading level0 col1\" >Skewness Description</th>\n",
       "      <th class=\"col_heading level0 col2\" >Mean/Mode Assumptions and Distribution Details</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_d930c_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_d930c_row0_col0\" class=\"data row0 col0\" >Income</td>\n",
       "      <td id=\"T_d930c_row0_col1\" class=\"data row0 col1\" >Positively skewed. Higher concentration of individuals with lower incomes, with fewer high-income earners.</td>\n",
       "      <td id=\"T_d930c_row0_col2\" class=\"data row0 col2\" >Mean: €4000, Distribution: Log-normal with σ = 0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d930c_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_d930c_row1_col0\" class=\"data row1 col0\" >Loan_Amount</td>\n",
       "      <td id=\"T_d930c_row1_col1\" class=\"data row1 col1\" >Positively skewed. Loan amounts are concentrated towards lower to moderate values, with fewer large loan amounts.</td>\n",
       "      <td id=\"T_d930c_row1_col2\" class=\"data row1 col2\" >Mean: €175,000, Distribution: Log-normal with σ = 0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d930c_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_d930c_row2_col0\" class=\"data row2 col0\" >Credit_Score</td>\n",
       "      <td id=\"T_d930c_row2_col1\" class=\"data row2 col1\" >Negatively skewed. Most people have good to excellent credit scores, with fewer individuals having very low credit scores.</td>\n",
       "      <td id=\"T_d930c_row2_col2\" class=\"data row2 col2\" >Mean: 700, Mode: 750, Distribution: Normal (reversed) with μ = 100 and σ = 50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d930c_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_d930c_row3_col0\" class=\"data row3 col0\" >Employment_Status</td>\n",
       "      <td id=\"T_d930c_row3_col1\" class=\"data row3 col1\" >Binary distribution. Higher proportion of the population being employed, heavily weighted towards employment.</td>\n",
       "      <td id=\"T_d930c_row3_col2\" class=\"data row3 col2\" >Mode: Employed, Distribution: Binary with p = 0.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d930c_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_d930c_row4_col0\" class=\"data row4 col0\" >Debt_to_Income</td>\n",
       "      <td id=\"T_d930c_row4_col1\" class=\"data row4 col1\" >Positively skewed. More individuals with lower ratios, but significant instances of high debt relative to income.</td>\n",
       "      <td id=\"T_d930c_row4_col2\" class=\"data row4 col2\" >Mean: 0.6, Distribution: Beta (α = 2, β = 5) scaled to [0, 1.2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d930c_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
       "      <td id=\"T_d930c_row5_col0\" class=\"data row5 col0\" >Loan_Term</td>\n",
       "      <td id=\"T_d930c_row5_col1\" class=\"data row5 col1\" >Discrete uniform. Distributed across specific intervals with peaks at common loan terms like 20 and 30 years.</td>\n",
       "      <td id=\"T_d930c_row5_col2\" class=\"data row5 col2\" >Mode: 20 years, Distribution: Discrete with p = [0.1, 0.2, 0.3, 0.1, 0.3]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d930c_level0_row6\" class=\"row_heading level0 row6\" >6</th>\n",
       "      <td id=\"T_d930c_row6_col0\" class=\"data row6 col0\" >Age</td>\n",
       "      <td id=\"T_d930c_row6_col1\" class=\"data row6 col1\" >Approximately normal. Centered around peak working ages (30-45 years), with fewer young and old applicants.</td>\n",
       "      <td id=\"T_d930c_row6_col2\" class=\"data row6 col2\" >Mean: 35, Distribution: Normal with μ = 35 and σ = 10, clipped to [18, 75]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d930c_level0_row7\" class=\"row_heading level0 row7\" >7</th>\n",
       "      <td id=\"T_d930c_row7_col0\" class=\"data row7 col0\" >Home_Ownership</td>\n",
       "      <td id=\"T_d930c_row7_col1\" class=\"data row7 col1\" >Binary distribution. Higher proportion of the population owning homes, heavily weighted towards ownership.</td>\n",
       "      <td id=\"T_d930c_row7_col2\" class=\"data row7 col2\" >Mode: Own, Distribution: Binary with p = 0.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d930c_level0_row8\" class=\"row_heading level0 row8\" >8</th>\n",
       "      <td id=\"T_d930c_row8_col0\" class=\"data row8 col0\" >Default</td>\n",
       "      <td id=\"T_d930c_row8_col1\" class=\"data row8 col1\" >Positively skewed. Default rates are typically low, with a small percentage representing defaults.</td>\n",
       "      <td id=\"T_d930c_row8_col2\" class=\"data row8 col2\" >Mean: 0.15, Distribution: Top 15% of risk scores</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x1989555e400>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from IPython.display import display\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import make_scorer, recall_score, classification_report,\n",
    "import logging\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "\n",
    "# Load the data\n",
    "with open('data/loan_data.pkl', 'rb') as file:\n",
    "    loaded_data = pickle.load(file)\n",
    "\n",
    "df = loaded_data['df']\n",
    "metadata_df = loaded_data['metadata_df']\n",
    "numeric_features = loaded_data['numeric_features']  # \n",
    "categorical_features = loaded_data['categorical_features']\n",
    "target_feature = loaded_data['target_feature']\n",
    "\n",
    "# Display metadata of the synthetic data distribution\n",
    "# Style the DataFrame for better readability\n",
    "styled_df = metadata_df.style.set_properties(**{'text-align': 'left'})\n",
    "styled_df.set_table_styles([dict(selector = 'th', props=[('text-align', 'left')])])\n",
    "\n",
    "# Display the styled DataFrame\n",
    "display(styled_df)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Income</th>\n",
       "      <th>Loan_Amount</th>\n",
       "      <th>Credit_Score</th>\n",
       "      <th>Debt_to_Income</th>\n",
       "      <th>Age</th>\n",
       "      <th>Employment_Status</th>\n",
       "      <th>Home_Ownership</th>\n",
       "      <th>Loan_Term</th>\n",
       "      <th>Default</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5127.670373</td>\n",
       "      <td>183807.505965</td>\n",
       "      <td>848.470271</td>\n",
       "      <td>0.331069</td>\n",
       "      <td>39</td>\n",
       "      <td>Employed</td>\n",
       "      <td>Own</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3732.813394</td>\n",
       "      <td>169479.480329</td>\n",
       "      <td>850.000000</td>\n",
       "      <td>0.193608</td>\n",
       "      <td>21</td>\n",
       "      <td>Employed</td>\n",
       "      <td>Rent</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5529.728012</td>\n",
       "      <td>281654.558991</td>\n",
       "      <td>850.000000</td>\n",
       "      <td>0.463659</td>\n",
       "      <td>44</td>\n",
       "      <td>Employed</td>\n",
       "      <td>Own</td>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8566.072041</td>\n",
       "      <td>376606.751901</td>\n",
       "      <td>850.000000</td>\n",
       "      <td>0.433488</td>\n",
       "      <td>44</td>\n",
       "      <td>Employed</td>\n",
       "      <td>Rent</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3558.067903</td>\n",
       "      <td>246708.985583</td>\n",
       "      <td>850.000000</td>\n",
       "      <td>0.222574</td>\n",
       "      <td>35</td>\n",
       "      <td>Employed</td>\n",
       "      <td>Own</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Income    Loan_Amount  Credit_Score  Debt_to_Income  Age  \\\n",
       "0  5127.670373  183807.505965    848.470271        0.331069   39   \n",
       "1  3732.813394  169479.480329    850.000000        0.193608   21   \n",
       "2  5529.728012  281654.558991    850.000000        0.463659   44   \n",
       "3  8566.072041  376606.751901    850.000000        0.433488   44   \n",
       "4  3558.067903  246708.985583    850.000000        0.222574   35   \n",
       "\n",
       "  Employment_Status Home_Ownership Loan_Term  Default  \n",
       "0          Employed            Own        30        0  \n",
       "1          Employed           Rent        30        0  \n",
       "2          Employed            Own        25        0  \n",
       "3          Employed           Rent        15        0  \n",
       "4          Employed            Own        20        0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Combine all features\n",
    "all_features = numeric_features + categorical_features + [target_feature]\n",
    "\n",
    "# Filter the dataframe\n",
    "df_filtered = df[all_features]\n",
    "\n",
    "df_filtered.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Form Random Forest Model\n",
    "\n",
    "#### 3.2.1 Data preparation and dataset splitting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = 42\n",
    "target_feature = 'Default'\n",
    "\n",
    "# Separate features (X) and target (y)\n",
    "X = df_filtered.drop(columns=[target_feature])\n",
    "y = df_filtered[target_feature]\n",
    "\n",
    "# Label Encoding for categorical features\n",
    "le = LabelEncoder()\n",
    "for col in categorical_features:  # Assuming you have a list of categorical features\n",
    "    X[col] = le.fit_transform(X[col])\n",
    "\n",
    "# Standardize numerical features \n",
    "scaler = StandardScaler()\n",
    "X[numeric_features] = scaler.fit_transform(X[numeric_features])  # Assuming you have a list of numerical features\n",
    "\n",
    "# Split into training + validation and test sets\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=random_seed, stratify=y\n",
    ")\n",
    "\n",
    "# Split the training + validation set into separate training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train_val, y_train_val, test_size=0.25, random_state=random_seed, stratify=y_train_val\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.2 Model training\n",
    "\n",
    "We train our Random Forest model with balanced class weights and grid search scoring based on class 1 (defaults) recall. For time, we run a simplified hyperparameter grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "[CV 1/5] END max_depth=5, min_samples_leaf=2, min_samples_split=2, n_estimators=200;, score=0.764 total time=   4.3s\n",
      "[CV 2/5] END max_depth=5, min_samples_leaf=2, min_samples_split=2, n_estimators=200;, score=0.730 total time=   5.4s\n",
      "[CV 3/5] END max_depth=5, min_samples_leaf=2, min_samples_split=2, n_estimators=200;, score=0.782 total time=   4.9s\n",
      "[CV 4/5] END max_depth=5, min_samples_leaf=2, min_samples_split=2, n_estimators=200;, score=0.744 total time=   5.0s\n",
      "[CV 5/5] END max_depth=5, min_samples_leaf=2, min_samples_split=2, n_estimators=200;, score=0.741 total time=   5.0s\n",
      "[CV 1/5] END max_depth=10, min_samples_leaf=2, min_samples_split=2, n_estimators=200;, score=0.695 total time=   7.9s\n",
      "[CV 2/5] END max_depth=10, min_samples_leaf=2, min_samples_split=2, n_estimators=200;, score=0.658 total time=   7.0s\n",
      "[CV 3/5] END max_depth=10, min_samples_leaf=2, min_samples_split=2, n_estimators=200;, score=0.710 total time=   6.8s\n",
      "[CV 4/5] END max_depth=10, min_samples_leaf=2, min_samples_split=2, n_estimators=200;, score=0.678 total time=   6.8s\n",
      "[CV 5/5] END max_depth=10, min_samples_leaf=2, min_samples_split=2, n_estimators=200;, score=0.660 total time=   6.8s\n",
      "Validation Accuracy: 0.7399\n",
      "Validation Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.74      0.83      8388\n",
      "           1       0.35      0.75      0.48      1612\n",
      "\n",
      "    accuracy                           0.74     10000\n",
      "   macro avg       0.65      0.74      0.65     10000\n",
      "weighted avg       0.84      0.74      0.77     10000\n",
      "\n",
      "Validation Recall for Class 1: 0.7463\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(filename='data/grid_search_log.txt', level=logging.INFO, \n",
    "                    format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "\"\"\"\n",
    "# Hyperparameter grid (adjust as needed)\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [5, 10, 15],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "# Simplified hyperparameter grid (adjust as needed)\n",
    "param_grid = {\n",
    "    'n_estimators': [200],\n",
    "    'max_depth': [5, 10],\n",
    "    'min_samples_split': [2],\n",
    "    'min_samples_leaf': [2]\n",
    "}\n",
    "\n",
    "# Custom scorer for recall of class 1 (defaulters)\n",
    "scorer = make_scorer(recall_score, pos_label=1)\n",
    "\n",
    "# Create and fit the grid search\n",
    "rf = RandomForestClassifier(random_state=random_seed, class_weight=\"balanced\")\n",
    "grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=5, scoring=scorer, verbose=3)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Retrieve the best parameters and refit the model on the entire training data\n",
    "best_params = grid_search.best_params_\n",
    "best_score = grid_search.best_score_\n",
    "\n",
    "# Train the final model with the best parameters\n",
    "model = RandomForestClassifier(**best_params, random_state=random_seed, class_weight=\"balanced\")\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluation on the validation set\n",
    "y_pred_val = model.predict(X_val)\n",
    "accuracy_val = accuracy_score(y_val, y_pred_val)\n",
    "report_val = classification_report(y_val, y_pred_val)\n",
    "recall_val = recall_score(y_val, y_pred_val, pos_label=1)\n",
    "\n",
    "print(f'Validation Accuracy: {accuracy_val:.4f}')\n",
    "print('Validation Classification Report:\\n', report_val)\n",
    "print(f'Validation Recall for Class 1: {recall_val:.4f}')\n",
    "\n",
    "# Log the best parameters and score\n",
    "logging.info(f\"Best Parameters: {best_params}\")\n",
    "logging.info(f\"Best Class 1 Recall Score: {best_score:.4f}\")\n",
    "\n",
    "# Log the full grid search results\n",
    "results_df = pd.DataFrame(grid_search.cv_results_)  # Convert to DataFrame for easier logging\n",
    "logging.info(\"Grid Search CV Results:\")\n",
    "for _, row in results_df.iterrows():  \n",
    "    logging.info(row.to_json())\n",
    "logging.shutdown()\n",
    "\n",
    "# Save the model\n",
    "with open('model/random_forest_model_train.pkl', 'wb') as file:\n",
    "    pickle.dump(model, file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.3 Model evaluation\n",
    "We note from the training log that larger max depth resulted in decreasing performance, suggesting overfitting. \n",
    "\n",
    "Further hyperparameter tunign might improve the model, but we leave that as a further improvement point. We also note that the results from our random forest are very similar to our logistic regression model, which may imply that we are reaching the best possible identifications based on the underlying synthetic data. \n",
    "\n",
    "We run our final model on the validation and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters: {'max_depth': 5, 'min_samples_leaf': 2, 'min_samples_split': 2, 'n_estimators': 200}, Mean Accuracy: 0.7522\n",
      "Parameters: {'max_depth': 10, 'min_samples_leaf': 2, 'min_samples_split': 2, 'n_estimators': 200}, Mean Accuracy: 0.6802\n"
     ]
    }
   ],
   "source": [
    "# Extract mean test scores (accuracy) for each parameter combination\n",
    "mean_test_scores = grid_search.cv_results_['mean_test_score']\n",
    "\n",
    "# Extract parameters for each combination\n",
    "params = grid_search.cv_results_['params']\n",
    "\n",
    "# Print results in a table format\n",
    "for score, param in zip(mean_test_scores, params):\n",
    "    if not np.isnan(score): # Filter out NaN values that result from the memory issue\n",
    "        print(f\"Parameters: {param}, Mean Accuracy: {score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.7399\n",
      "Validation Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.74      0.83      8388\n",
      "           1       0.35      0.75      0.48      1612\n",
      "\n",
      "    accuracy                           0.74     10000\n",
      "   macro avg       0.65      0.74      0.65     10000\n",
      "weighted avg       0.84      0.74      0.77     10000\n",
      "\n",
      "Validation Recall for Class 1: 0.7463\n",
      "Test Accuracy: 0.7297\n",
      "Test Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.73      0.82      8388\n",
      "           1       0.34      0.74      0.47      1612\n",
      "\n",
      "    accuracy                           0.73     10000\n",
      "   macro avg       0.64      0.73      0.64     10000\n",
      "weighted avg       0.84      0.73      0.76     10000\n",
      "\n",
      "Test Recall for Class 1: 0.7413\n"
     ]
    }
   ],
   "source": [
    "# Train the final model with best parameters\n",
    "model = RandomForestClassifier(**best_params, random_state=random_seed, class_weight=\"balanced\")\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# Evaluation on the validation set\n",
    "y_pred_val = model.predict(X_val)\n",
    "accuracy_val = accuracy_score(y_val, y_pred_val)\n",
    "report_val = classification_report(y_val, y_pred_val)\n",
    "recall_val = recall_score(y_val, y_pred_val, pos_label=1)\n",
    "\n",
    "print(f'Validation Accuracy: {accuracy_val:.4f}')\n",
    "print('Validation Classification Report:\\n', report_val)\n",
    "print(f'Validation Recall for Class 1: {recall_val:.4f}')\n",
    "\n",
    "# Log the best parameters and score\n",
    "best_params = grid_search.best_params_\n",
    "best_score = grid_search.best_score_\n",
    "logging.info(f\"Best Parameters: {best_params}\")\n",
    "logging.info(f\"Best Class 1 Recall Score: {best_score:.4f}\")\n",
    "\n",
    "# Log the full grid search results\n",
    "results_df = pd.DataFrame(grid_search.cv_results_)  # Convert to DataFrame for easier logging\n",
    "logging.info(\"Grid Search CV Results:\")\n",
    "for _, row in results_df.iterrows():  \n",
    "    logging.info(row.to_json())\n",
    "logging.shutdown()\n",
    "\n",
    "# Save the model\n",
    "with open('model/random_forest_model_valid.pkl', 'wb') as file:\n",
    "    pickle.dump(model, file)\n",
    "\n",
    "# Evaluation on the test set\n",
    "y_pred_test = model.predict(X_test)\n",
    "accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "report_test = classification_report(y_test, y_pred_test)\n",
    "recall_test = recall_score(y_test, y_pred_test, pos_label=1)\n",
    "\n",
    "print(f'Test Accuracy: {accuracy_test:.4f}')\n",
    "print('Test Classification Report:\\n', report_test)\n",
    "print(f'Test Recall for Class 1: {recall_test:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We note that the test and validation sets display remarkable consistency in results, which are also in line with our linear regression model.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Analysis of Validation Results\n",
    "\n",
    "- **Overall Accuracy**: The validation accuracy is 73.99%, indicating that approximately three-quarters of the predictions were correct.\n",
    "- **Class 0 (Non-defaulters)**:\n",
    "  - **Precision**: High at 94%, indicating the model is very accurate when it predicts a non-defaulter.\n",
    "  - **Recall**: Moderate at 74%, meaning the model correctly identifies a significant portion of non-defaulters.\n",
    "  - **F1-Score**: Strong at 0.83, reflecting a good balance between precision and recall for this class.\n",
    "- **Class 1 (Defaulters)**:\n",
    "  - **Precision**: Lower at 35%, indicating a relatively high number of false positives.\n",
    "  - **Recall**: High at 75%, showing the model's effectiveness in identifying a substantial proportion of actual defaulters.\n",
    "  - **F1-Score**: Moderate at 0.48, suggesting room for improvement in balancing precision and recall for defaulters.\n",
    "- **Macro and Weighted Averages**:\n",
    "  - **Macro Average F1-Score**: At 0.65, reflecting overall performance across both classes without considering class imbalance.\n",
    "  - **Weighted Average F1-Score**: At 0.77, indicating the model's performance is more influenced by the majority class (non-defaulters).\n",
    "\n",
    "We propose that our key interest is class 1 recall, which is useful at 75% but surely has space for improvement. This is a highly similar result to our logistic regression model, which might suggest a best possible result given our synthetic dataset with limited features and manufactured randomness and correlations. \n",
    "\n",
    "However, we will mention possible improvement methods below: \n",
    "\n",
    "\n",
    "### 3.4 Possible Next Steps for Better Results\n",
    "\n",
    "#### Improving Results with Random Forest\n",
    "\n",
    "1. **Hyperparameter Tuning**:\n",
    "   - **Increase Tree Count**: Raise `n_estimators` to ensure the model benefits from a larger ensemble (e.g., `[100, 200, 300]`).\n",
    "   - **Optimize Tree Depth**: Experiment with different values for `max_depth` to prevent overfitting and underfitting (e.g., `[3, 5, 7, 10]`).\n",
    "   - **Adjust Splitting Criteria**: Tune `min_samples_split` and `min_samples_leaf` to control the complexity of the trees (e.g., `[2, 5, 10]` for `min_samples_split` and `[1, 2, 4]` for `min_samples_leaf`).\n",
    "\n",
    "2. **Class Imbalance Handling**:\n",
    "   - **Over-sampling/Under-sampling**: Apply techniques such as SMOTE (Synthetic Minority Over-sampling Technique) or under-sampling the majority class to balance the dataset.\n",
    "   - **Cost-sensitive Learning**: Integrate class weights directly into the model to penalize misclassification of the minority class more heavily.\n",
    "\n",
    "3. **Feature Engineering**:\n",
    "   - **Create Interaction Features**: Introduce features that capture interactions between existing features.\n",
    "   - **Polynomial Features**: Generate polynomial features to capture non-linear relationships.\n",
    "\n",
    "#### Improving Overall Results\n",
    "\n",
    "1. **Feature Engineering** (if using a real life dataset with richer features):\n",
    "   - **Domain-Specific Features**: Develop new features based on domain knowledge, such as ratios, trends, or categorical groupings.\n",
    "   - **Feature Selection**: Use techniques such as Recursive Feature Elimination (RFE) to identify and retain the most significant features.\n",
    "   - **Normalization/Standardization**: Ensure features are appropriately scaled to enhance model performance.\n",
    "\n",
    "2. **Other Models**:\n",
    "   - **Gradient Boosting Machines (GBM)**: Models like XGBoost, LightGBM, and CatBoost may outperform random forests by focusing on correcting the mistakes of previous models.\n",
    "   - **Neural Networks**: Implement deep learning models for their ability to capture complex patterns in large datasets.\n",
    "\n",
    "3. **Cross-Validation**:\n",
    "   - **K-Fold Cross-Validation**: Use k-fold cross-validation to better assess model performance and ensure it generalizes well to unseen data.\n",
    "   - **Stratified Sampling**: Ensure each fold in cross-validation maintains the same class distribution as the original dataset.\n",
    "\n",
    "Implementing these strategies could help improve the model's performance, especially in correctly identifying defaulters, and ensure a more balanced and robust predictive system.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
